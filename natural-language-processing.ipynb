{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a collection of useful functions and code snippets for Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "\n",
    "1. [Text preprocessing](#Text-preprocessing)<br>\n",
    "    1.1 [Word tokenization](#Word-tokenization)<br>\n",
    "    1.2 [Removal of stop words](#Removal-of-stop-words)<br>\n",
    "    1.3 [Stemming](#Stemming)<br>\n",
    "    1.4 [Lemmatization](#Lemmatization)<br>\n",
    "    1.5 [Reprocessing function](#Preprocessing-function)<br>\n",
    "2. [Tokenization](#Tokenization)<br>\n",
    "3. [Word embeddings](#Word-embeddings)<br>\n",
    "    3.1 [GloVe](#GloVe)<br>\n",
    "    3.2 [BERT](#BERT)<br>\n",
    "    3.3 [Custom word embeddings](#Custom-word-embeddings)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text preprocessing<a id=\"Text-preprocessing\"/>\n",
    "Data preprocessing – or text preprocessing, in the context of NLP – is the practise of transforming data into a more digestable format for a machine learning model. Preprocessing is an essential step of the NLP workflow, and can have a significant impact on the outcome, of any particular model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Word tokenization<a id=\"Word-tokenization\"/>\n",
    "Word tokenization is the process of splitting a larger piece of text into smaller parts called tokens. For instance, a sentence into words. The output of tokenization is used as the input for many other preprocessing tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Splitting', 'the', 'sentence', 'into', 'words', '.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"Splitting the sentence into words.\"\n",
    "words = word_tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Removal of stop words<a id=\"Removal-of-stop-words\"/>\n",
    "Stop words are a set of commonly used words such as \"a\", \"an\", \"the\", \"in\", etc. By convention, these words are generally removed, since they usually provide little added value in the context of most NLP projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Removal stop words'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentence = \"Removal of stop words\"\n",
    "tokens = word_tokenize(sentence)\n",
    "tokens_without_stopwords = [word for word in tokens if not word in stopwords.words(\"english\")]\n",
    "sentence = (\" \").join(tokens_without_stopwords)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Stemming<a id=\"Stemming\"/>\n",
    "Stemming is the act of reducing a word to its stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stem is the act of reduc a word to it stem .'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"Stemming is the act of reducing a word to its stem.\"\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "tokens = word_tokenize(sentence)\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "sentence = (\" \").join(stemmed_tokens)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Lemmatization<a id=\"Lemmatization\"/>\n",
    "Lemmatizing is the act of reducing a word to its dictionary root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lemmatizing is the act of reducing a word to it dictionary root form .'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"Lemmatizing is the act of reducing a word to its dictionary root form.\"\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = word_tokenize(sentence)\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "sentence = (\" \").join(lemmatized_tokens)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Preprocessing function<a id=\"Preprocessing-function\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'short meaningless sentencce testing preprocess text function'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "sentence = \"A short meaningless sentencce for the testing of the below preprocess_text function.\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    sentence = text.lower()\n",
    "    \n",
    "    # Remove emails\n",
    "    sentence = re.sub(r\"@[^\\s]+\", \" \", sentence)\n",
    "    \n",
    "    # Remove URLs\n",
    "    sentence = re.sub(r\"((www\\.[^\\s]+)|(https?://[^\\s]+))\", \" \", sentence)\n",
    "    \n",
    "    # Remove tags\n",
    "    sentence = re.sub(r\"<[^>]*>\", \" \", sentence)\n",
    "    \n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "    \n",
    "    # Remove single characters \n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", sentence)\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens_without_stopwords = [word for word in tokens if not word in stopwords.words(\"english\")]\n",
    "    sentence = (\" \").join(tokens_without_stopwords)\n",
    "    \n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_tokens = word_tokenize(sentence)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in lemma_tokens]\n",
    "    sentence = (\" \").join(lemmatized_tokens)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "sentence = preprocess_text(sentence)\n",
    "sentence\n",
    "# X = df[\"Text\"].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization<a id=\"Tokenization\"/>\n",
    "Tokenization is a way of converting textual data (sentences) into a numeric representation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 1, 2], [4, 1, 2]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\n",
    "    \"An arbitrary sentence\",\n",
    "    \"Another arbitrary sentence\"  \n",
    "]\n",
    "\n",
    "# Create an instance of the Tokenizer object\n",
    "tokenizer = Tokenizer(num_words=100) # num_words sets the maximum number of words to keep\n",
    "\n",
    "# Fit tokenizer on your sentences\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn sentences into sequences of numbers\n",
    "X = tokenizer.texts_to_sequences(sentences)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word Embeddings<a id=\"Word-embeddings\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embedding is the act of transforming words, which are essentially lists of characters, into a computer understandable format - namely vector representations of words. Word embeddings takes into account the semantic relations between words, and result in more condensed representations, in contrast to methods such as one-hot encoding.\n",
    "\n",
    "There are different methods for using word embeddings. Using pre-trained models such as GloVe, BERT or creating custom word embeddings, to name a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 GloVe<a id=\"GloVe\"/>\n",
    "<a ahref =\"https://nlp.stanford.edu/projects/glove/\">GloVe: Global Vectors for Word Representation</a> offers several pre-trained word embeddings, including embeddings specifically trained on tweets.\n",
    "* Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download): glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a word embeddings dictionary; where keys represent the words, and values the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Create empty dictionary\n",
    "embeddings_dict = dict()\n",
    "\n",
    "GLOVE_DIM = 200 # Dimensions of the word embeddings\n",
    "\n",
    "# Filepath + file\n",
    "file = \"data/\" + \"glove.twitter.27B.\" + str(GLOVE_DIM) + \"d.txt\"\n",
    "\n",
    "# Populate with words and embeddings\n",
    "with open(file, \"r\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the GloVe embeddings have been loaded into a dictionary, the embeddings matrix can be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings matrix\n",
    "embeddings_matrix = np.zeros((len(tokenizer.word_index) + 1, GLOVE_DIM)) # len(tokenizer.word_index refers to vocabulary size\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embeddings_vector = embeddings_dict.get(word)\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[index] = embeddings_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 BERT<a id=\"BERT\"/>\n",
    "Unlike the GloVe model, which is context independent, the BERT model takes into account the contexts of words.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Custom word embeddings<a id=\"Custom-word-embeddings\"/>\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
